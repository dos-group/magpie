import os
os.environ["CUDA_VISIBLE_DEVICES"] = "1"
import tensorflow as tf
print(tf.test.is_gpu_available())
import logging
import socket
import time
from typing import List

import typer
from devtools import pformat
from tf_agents.bandits.agents.examples.v2.trainer import tf
from tf_agents.drivers import dynamic_step_driver
from tf_agents.eval.metric_utils import log_metrics
from tf_agents.utils import common

from magpie.config.config import FioSettings, LustreKnobsSettings
from magpie.tuner.eval import Evaluation
from magpie.types.dfs_configuration import ConfigurationScorePair
from magpie.types.distributed_file_system import DFS
from magpie.types.reward_type import RewardType
from magpie.types.rl_model import RLModel
from magpie.utils import magpie_logging, commons, metrics_utils
from magpie.utils.commons import APP_ROOT
from magpie.utils.tuner_utils import save_configuration_score_pairs, create_model, evaluate_configuration, create_env


def main(
        # workload: Workload = typer.Option(...,show_choices=, help="Workload generated by Fio"),
        dfs: DFS = typer.Option(DFS.LUSTRE.value, help="distributed file system type"),
        model: RLModel = typer.Option(RLModel.DDPG.value, help="RL model"),
        reward_type: RewardType = typer.Option(RewardType.REWARD_2A.value, help="reward type "),
        num_iterations: int = typer.Option(..., help="Number of training iterations"),
        num_eval_iterations: int = typer.Option(2, help="Number of evaluation iterations"),
        num_init_steps: int = typer.Option(2, help="Number of initial data collection steps"),
        train_step_per_collection: int = typer.Option(3,
                                                      help="Number of training iteration after each collection step"),
        num_collect_steps_per_run: int = typer.Option(1, help="Number of steps in each collector run"),
        save_interval: int = typer.Option(1000, help="Interval of model saving frequency"),
        exploration_return_toleration: int = typer.Option(-6000,
                                                          help="minimum sum return of the agent. if it is below this level, the training stops."),
        enable_observation_normalizer: bool = typer.Option(False, help="Normalize observation"),
        observation_time: int = typer.Option(15, help="Observation time in each environment step"),
        evaluation_time: int = typer.Option(None,
                                            help="Evaluation time for each top configurations in the final phase"),
        debug: bool = typer.Option(False, help="Running in debug mode"),
        eval_step: int = typer.Option(10, help="Evaluate the mode every eval_step"),
        periodic_workload: bool = typer.Option(True, help="tuning between each run of the workload"),
        double_optimization: bool = typer.Option(False, help="optimize double performance indicator"),
        experiment_name: str = typer.Option(..., help="experiment name")
):
    # workload = Workload.FINAL_RW
    workload = None
    current_time = time.strftime("%Y%m%d-%H%M%S")
    norm_arg = "normalization_y" if enable_observation_normalizer else "normalization_x"
    num_knob = f"{len(LustreKnobsSettings().get_all_knobs())}knobs" if dfs.value is DFS.LUSTRE.value else ""
    workload_str = workload.__str__() if workload is not None else "manual_workload"
    workload_str += "_2op" if double_optimization else "_1op"
    train_log_dir = f"{APP_ROOT}/log/train/{experiment_name}_{current_time}"
    # eavl_log_dir = f"{APP_ROOT}/log/eval/{experiment_name}_{current_time}"
    checkpoint_dir = f"{APP_ROOT}/checkpoint/{experiment_name}"
    summaries_flush_secs = 5
    is_pro_env = "wally" in socket.gethostname()
    magpie_logging.init(train_log_dir, is_pro_env)
    logger = logging.getLogger(__name__)
    # Create Environment
    global_step = tf.compat.v1.train.get_or_create_global_step()
    tf.summary.experimental.set_step(global_step)
    logging.info(f"train logger is {pformat(logger)}")
    internal_pis, knobs, py_environment, tf_env = create_env(debug, dfs,
                                                             enable_observation_normalizer,
                                                             observation_time, reward_type, workload, model,
                                                             dd_workload=periodic_workload, double_optimization=double_optimization)

    # Build models
    model, model_settings = create_model(global_step, model, tf_env)
    train_checkpointer = common.Checkpointer(
        ckpt_dir=checkpoint_dir,
        max_to_keep=10,
        agent=model.tf_agent,
        policy=model.tf_agent.policy,
        replay_buffer=model.replay_buffer,
        global_step=global_step
    )
    train_checkpointer.initialize_or_restore()
    title_width = 50
    configuration_dump = "".center(title_width, "-") + "\n" + \
                         f"Workload {workload_str}" + "\n" + \
                         f"Model {model}" + "\n" + \
                         f"num_knobs {num_knob}" + "\n" + \
                         f"norm_args {norm_arg}" + "\n" + \
                         f"reward_type {reward_type}" + "\n" + \
                         "Tuning Settings Dump Start".center(title_width, "-") + "\n" + \
                         f"Tensorflow log dir: {train_log_dir}\n" + \
                         f"internal performance indicators (used as state): {pformat(internal_pis)}\n" + \
                         f"knobs to tune: {pformat(knobs)}\n" + \
                         "Tuning Settings Dump End".center(title_width, "-") + "\n" + \
                         "".center(title_width, "-")
    logger.info(configuration_dump)

    sum_of_return_metrics = metrics_utils.TFSumReturnMetric()
    train_metrics = [
        sum_of_return_metrics,
    ]
    train_summary_writer = tf.compat.v2.summary.create_file_writer(
        train_log_dir, flush_millis=summaries_flush_secs * 1000)
    # eval_summary_writer = tf.compat.v2.summary.create_file_writer(
    #     eval_log_dir, flush_millis=summaries_flush_secs * 1000)
    train_summary_writer.set_as_default()
    initial_collect_driver = dynamic_step_driver.DynamicStepDriver(
        tf_env,
        policy=model.tf_agent.collect_policy,
        observers=[model.replay_buffer.add_batch],
        num_steps=num_init_steps)

    collect_driver = dynamic_step_driver.DynamicStepDriver(
        tf_env,
        model.tf_agent.collect_policy,
        observers=[model.replay_buffer.add_batch] + train_metrics,
        num_steps=num_collect_steps_per_run)

    logger.info(f"Initialize environment.")
    tf_env.reset()
    logger.info(f"Collect initial data.")
    initial_collect_driver.run()
    logger.info(f"Finish initial data collection")
    logger.info(f"Start tuning")
    dataset_iterator = model.get_dataset_iterator(3, tf_env.batch_size, 2, 3)
    time_step = None
    policy_state = model.tf_agent.collect_policy.get_initial_state(tf_env.batch_size)

    for iteration_no in range(1, num_iterations):
        step_time = commons.time_start()
        action_step_time = commons.time_start()
        time_step, policy_state = collect_driver.run(
            time_step=time_step,
            policy_state=policy_state,
            maximum_iterations=1
        )

        action_step_time = commons.time_end(action_step_time)
        train_step_time = commons.time_start()
        for _ in range(train_step_per_collection):
            experience, _ = next(dataset_iterator)
            training_loss = model.train(experience)
        train_step_time = commons.time_end(train_step_time)

        for train_metric in train_metrics:
            train_metric.tf_summaries(train_step=global_step, step_metrics=train_metrics[:2])
        log_metrics(train_metrics)
        step_time = commons.time_end(step_time)
        logger.info(
            f"[Iteration: {iteration_no}] training_loss: {training_loss.loss:.2f} step: {step_time:.2f}s train step: {train_step_time:.2f}s action time: {action_step_time:.2f}s")
        tf.summary.scalar("training/train_step_time", train_step_time, global_step)
        tf.summary.scalar("training/action_step_time", action_step_time, global_step)

        # evaluation
        if eval_step is not None and iteration_no % eval_step == 0:
            logger.info("---Evaluation Start---")
            good_configurations: List[ConfigurationScorePair] = py_environment.good_configurations
            evaluate_configuration(train_log_dir, good_configurations, py_environment, evaluation_time)
            logger.info("---Evaluation End---")

        # save network
        if iteration_no % save_interval == 0:
            train_checkpointer.save(global_step)
        sum_of_return = sum_of_return_metrics.result().numpy()
        if global_step.numpy() > 1 / 2 * num_iterations * train_step_per_collection and \
                sum_of_return < exploration_return_toleration:
            logger.warning(
                f"Stop training as the agent exceeds return sum toleration, current return sum: {sum_of_return}")
            break
        if periodic_workload and global_step.numpy() % 20 == 0:
            logger.info(f"Best configuration so far: {pformat(py_environment.good_configurations)}")

    # evaluation
    logger.info("Start evaluation")
    fio_settings = FioSettings()
    fio_settings.workload = workload
    Evaluation(tf_env, model.tf_agent.policy, num_eval_iterations, fio_settings).test()

    # evaluate and select best configuration among best_configurations and the last configuration recommend by the model
    good_configurations: List[ConfigurationScorePair] = py_environment.good_configurations
    save_configuration_score_pairs(train_log_dir, good_configurations, "good_configurations")
    if periodic_workload:
        evaluation_time = None
    evaluate_configuration(train_log_dir, good_configurations, py_environment, evaluation_time)


if __name__ == '__main__':
    typer.run(main)
